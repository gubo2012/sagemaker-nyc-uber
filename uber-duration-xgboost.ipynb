{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker XGBoost model applied to NYC Uber data\n",
    "\n",
    "Our SageMaker XGBoost regression model predicts trip duration based on feature vector that includes source zone, destination zone, and month, day and hour for the pickup time. \n",
    "\n",
    "The first step in using SageMaker is to create a SageMaker execution role that contains permissions used by SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "# Create SageMaker role \n",
    "role = get_execution_role()\n",
    "\n",
    "# get the url to the container image for using linear-learner\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "xgboost_image = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "print(xgboost_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data from CSV to protobuf recordIO Format\n",
    "\n",
    "We have multiple CSV files available in S3 bucket. Each CSV file contains numeric columns for encoded origin zone, encoded destination zone, month, day, hour, trip distance in miles and trip duration in seconds. \n",
    "\n",
    "We will download the CSV files available in the source S3 bucket, split the csv files into training, validation and test data sets for SageMaker training and upload them to destination bucket to stage them for SageMaker training input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import csv\n",
    "\n",
    "\n",
    "# source bucket with CSV files\n",
    "source_bucket='<source-s3-bucket>'\n",
    "source_prefix='glue/output/uber_nyc'\n",
    "\n",
    "# destination bucket to upload SageMaker training input data files\n",
    "dest_bucket='<destination-s3-bucket>'\n",
    "dest_prefix = 'sagemaker/input/uber_nyc'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "response=s3.list_objects_v2(Bucket=source_bucket, Prefix=source_prefix)\n",
    "contents=response['Contents']\n",
    "count=len(contents)\n",
    "\n",
    "sbucket = boto3.resource('s3').Bucket(source_bucket)\n",
    "ntrain=int(count*0.80)\n",
    "nval = int(count*0.18)\n",
    "ntest = count - ntrain - nval\n",
    "\n",
    "def stage_data(start, end, name):  \n",
    "    for i in range(start, end, 1):\n",
    "        item=contents[i]\n",
    "        key =item['Key']    \n",
    "        with tempfile.NamedTemporaryFile(mode='w+b', suffix='.csv', prefix='data-', delete=True) as csv_file:\n",
    "            print(f'Download {key} file')\n",
    "            sbucket.download_fileobj(key, csv_file)\n",
    "            \n",
    "            with open(csv_file.name, 'rb') as data_reader:\n",
    "                dest_key = f'{dest_prefix}/{name}/part-{i}.csv'\n",
    "                print(f'upload {dest_key} file')\n",
    "                s3.upload_fileobj(data_reader, dest_bucket, dest_key)\n",
    "                data_reader.close()\n",
    "        \n",
    "            csv_file.close()\n",
    "            \n",
    "stage_data(0, ntrain, 'train')\n",
    "stage_data(ntrain, ntrain+nval, 'validation')\n",
    "stage_data(ntrain+nval, count, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data input channels\n",
    "\n",
    "We will create train, validaiton and test input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import s3_input\n",
    "\n",
    "s3_train = s3_input(s3_data=f's3://{dest_bucket}/{dest_prefix}/train', content_type='csv')\n",
    "s3_validation = s3_input(s3_data=f's3://{dest_bucket}/{dest_prefix}/validation', content_type='csv')\n",
    "s3_test = s3_input(s3_data=f's3://{dest_bucket}/{dest_prefix}/test', content_type='csv')\n",
    "\n",
    "output_path=f's3://{dest_bucket}/sagemaker/output/uber_nyc/xgboost'\n",
    "data_channels = {'train': s3_train, 'validation': s3_validation, 'test': s3_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SageMaker XGBoost Estimator\n",
    "\n",
    "SageMaker Estimator class defines the SageMaker job for training XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "\n",
    "xgb = Estimator(image_name=xgboost_image,\n",
    "                            role=role, \n",
    "                            train_instance_count=1, \n",
    "                            train_instance_type='ml.c5.9xlarge',\n",
    "                            output_path=output_path,\n",
    "                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "xgb.set_hyperparameters(objective='reg:linear',\n",
    "                       grow_policy='depthwise')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SageMaker XGBoost Hyper-parameter Tuner ###\n",
    "Before we train the model, we will tune the hyperparamters. Below we will tune two hyper-paramters, 'num_round' and 'max_depth'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.tuner import CategoricalParameter\n",
    "\n",
    "objective_metric_name = \"validation:rmse\"\n",
    "\n",
    "num_round = IntegerParameter(10,100)\n",
    "max_depth = IntegerParameter(8,32)\n",
    "hyperparameter_ranges={}\n",
    "hyperparameter_ranges['num_round'] = num_round\n",
    "hyperparameter_ranges['max_depth'] = max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a hyper-parameter tuner that will use Bayesian search to minimize validation Root Mean Squere Error objective. We limit the maximum total hyper-parameter tuning jobs to 30 and concurrent jobs to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tuner=HyperparameterTuner(xgb, \n",
    "                                         objective_metric_name, \n",
    "                                         hyperparameter_ranges, \n",
    "                                         strategy='Bayesian', \n",
    "                                         objective_type='Minimize', \n",
    "                                         max_jobs=30, \n",
    "                                         max_parallel_jobs=5, \n",
    "                                         base_tuning_job_name='xgboost-tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Hyper-parameter Tuning ###\n",
    "Below we launch the hyper-parameter tuner. You must use SageMaker console to monitor hyper-parameter tuning jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_tuner.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the best hyper-paramters found in SageMaker console to set the hyper-parameters for the training estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(objective='reg:linear',\n",
    "                        grow_policy='depthwise',\n",
    "                        num_round=89, \n",
    "                        max_depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training Job ###\n",
    "Below we launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete we deploy the training model to a SageMaker endpoint. This step can take a few minutes, so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1, \n",
    "               instance_type='ml.t2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model to Endpoint ###\n",
    "Below we import the classes to make prediction with test data. These classes are used to serialize and de-serialize the data to SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions for Test Data ###\n",
    "Below we download the test data and submit the test data to SageMaker deployed endpoint to make predctions on test data and compare the predictions to expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket = boto3.resource('s3').Bucket(dest_bucket)\n",
    "with tempfile.NamedTemporaryFile(mode='w+b', suffix='.csv', prefix='data-', delete=True) as test_csv:\n",
    "    key=f'{dest_prefix}/test/part-20.csv'\n",
    "    print(f'download: {key}')\n",
    "    bucket.download_fileobj(key, test_csv)\n",
    "    print(\"read test csv file\")\n",
    "    array = np.genfromtxt(test_csv.name, delimiter=',', skip_header=False)\n",
    "    np.random.shuffle(array)\n",
    "    for i in range(100):\n",
    "        print(f'test input: {array[i, 1:]}')\n",
    "        result = xgb_predictor.predict(array[i, 1:])\n",
    "        print(f'predicted: {result}')\n",
    "        print(f'expected: {array[i,0]}')\n",
    "    test_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
